{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dTX1iEFd4l2WdSQ5vLAHfWuFVvAHhwzt",
      "authorship_tag": "ABX9TyOaR+bA/CJrLM503sidfPB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaheshan/Breast_Cancer_Prediction_Model/blob/main/notebook03final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Title: Final Python Notebook 3 – Ensemble Classifier & DT Regression\n",
        "# Author: Suresh Thaheshan\n",
        "# Peer Reviewer: [Peer Reviewer Name], Date: [Review Date]\n",
        "# Reused from: Code Reuse Session 3\n",
        "# =============================================================\n",
        "\n",
        "# ============ IMPORT LIBRARIES ============\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, auc,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import os\n",
        "\n",
        "# ============ LOAD DATA ============\n",
        "# Load preprocessed datasets\n",
        "classification_df = pd.read_csv('/content/drive/MyDrive/Machine_Learning_CourseWork/classification_dataset.csv')\n",
        "regression_df = pd.read_csv('/content/drive/MyDrive/Machine_Learning_CourseWork/regression_dataset2.csv')\n",
        "\n",
        "# ============ PREPROCESSING ============\n",
        "\n",
        "# For classification task\n",
        "X_class = classification_df.drop(columns=['Mortality_Status'])\n",
        "y_class = classification_df['Mortality_Status']\n",
        "\n",
        "# For regression task - ensure the 'Survival_Months' column exists\n",
        "if 'Survival_Months' in regression_df.columns:\n",
        "    X_reg = regression_df.drop(columns=['Survival_Months'])\n",
        "    y_reg = regression_df['Survival_Months']\n",
        "else:\n",
        "    print(\"Column 'Survival_Months' not found in the dataset.\")\n",
        "    y_reg = None  # Prevent further errors\n",
        "\n",
        "# Train-Test Split for Classification\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_class, y_class, test_size=0.2, stratify=y_class, random_state=42)\n",
        "\n",
        "# Train-Test Split for Regression (only if 'Survival_Months' column exists)\n",
        "if y_reg is not None:\n",
        "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============ ENSEMBLE CLASSIFIER ============\n",
        "\n",
        "# Define base learners\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Ensemble using VotingClassifier (soft voting)\n",
        "ensemble_model = VotingClassifier(estimators=[('lr', lr), ('nb', nb)], voting='soft')\n",
        "\n",
        "# Train ensemble\n",
        "ensemble_model.fit(Xc_train, yc_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred_ens = ensemble_model.predict(Xc_test)\n",
        "y_prob_ens = ensemble_model.predict_proba(Xc_test)[:, 1]\n",
        "\n",
        "# Confusion matrix & classification report\n",
        "print(\"Confusion Matrix (Ensemble):\")\n",
        "print(confusion_matrix(yc_test, y_pred_ens))\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(yc_test, y_pred_ens))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(yc_test, y_prob_ens)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Ensemble ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============ REGRESSION – DECISION TREE MODELS ============\n",
        "\n",
        "# Model 1: Fully grown Decision Tree\n",
        "dt1 = DecisionTreeRegressor(random_state=42)\n",
        "dt1.fit(Xr_train, yr_train)\n",
        "\n",
        "# Model 2: Pruned Decision Tree (max_depth=4)\n",
        "dt2 = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt2.fit(Xr_train, yr_train)\n",
        "\n",
        "# ============ EVALUATION – REGRESSION MODELS ============\n",
        "\n",
        "def evaluate_regression(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation:\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "    print(f\"R-squared: {r2:.4f}\")\n",
        "\n",
        "    # Plotting actual vs predicted values\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.title(f'{model_name} – Actual vs Predicted')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "evaluate_regression(dt1, Xr_test, yr_test, 'Fully Grown Decision Tree')\n",
        "evaluate_regression(dt2, Xr_test, yr_test, 'Pruned Decision Tree')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nMWKvz9STA-_",
        "outputId": "5e205513-e7cc-425b-eb76-527dbb4bcada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'Survival_Months' not found in the dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a1e781229d55>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Train-Test Split for Classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mXc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Train-Test Split for Regression (only if 'Survival_Months' column exists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2403\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m             )\n\u001b[0;32m-> 2405\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Title: Final Python Notebook 3 – Ensemble Classifier & DT Regression\n",
        "# Author: Suresh Thaheshan\n",
        "# Peer Reviewer: [Peer Reviewer Name], Date: [Review Date]\n",
        "# Reused from: Code Reuse Session 3\n",
        "# =============================================================\n",
        "\n",
        "# ============ IMPORT LIBRARIES ============\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, auc,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import os\n",
        "\n",
        "# ============ LOAD DATA ============\n",
        "# Load preprocessed datasets\n",
        "classification_df = pd.read_csv('/content/drive/MyDrive/Machine_Learning_CourseWork/classification_dataset.csv')\n",
        "regression_df = pd.read_csv('/content/drive/MyDrive/Machine_Learning_CourseWork/regression_dataset.csv')\n",
        "\n",
        "# ============ PREPROCESSING ============\n",
        "# For classification task\n",
        "X_class = classification_df.drop(columns=['Mortality_Status'])\n",
        "y_class = classification_df['Mortality_Status']\n",
        "\n",
        "# For regression task\n",
        "if 'Survival_Months' in regression_df.columns:\n",
        "    X_reg = regression_df.drop(columns=['Survival_Months'])\n",
        "    y_reg = regression_df['Survival_Months']\n",
        "else:\n",
        "    print(\"Column 'Survival_Months' not found in the dataset.\")\n",
        "\n",
        "# Train-Test Split for Classification\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_class, y_class, test_size=0.2, stratify=y_class, random_state=42)\n",
        "\n",
        "# Train-Test Split for Regression\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============ ENSEMBLE CLASSIFIER ============\n",
        "# Define base learners\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "nb = GaussianNB()\n",
        "\n",
        "# Ensemble using VotingClassifier (soft voting)\n",
        "ensemble_model = VotingClassifier(estimators=[('lr', lr), ('nb', nb)], voting='soft')\n",
        "\n",
        "# Train ensemble\n",
        "ensemble_model.fit(Xc_train, yc_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred_ens = ensemble_model.predict(Xc_test)\n",
        "y_prob_ens = ensemble_model.predict_proba(Xc_test)[:, 1]\n",
        "\n",
        "# Confusion matrix & classification report\n",
        "print(\"Confusion Matrix (Ensemble):\")\n",
        "print(confusion_matrix(yc_test, y_pred_ens))\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(yc_test, y_pred_ens))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(yc_test, y_prob_ens)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Ensemble ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============ REGRESSION – DECISION TREE MODELS ============\n",
        "# Model 1: Fully grown Decision Tree\n",
        "dt1 = DecisionTreeRegressor(random_state=42)\n",
        "dt1.fit(Xr_train, yr_train)\n",
        "\n",
        "# Model 2: Pruned Decision Tree (max_depth=4)\n",
        "dt2 = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt2.fit(Xr_train, yr_train)\n",
        "\n",
        "# ============ EVALUATION – REGRESSION MODELS ============\n",
        "def evaluate_regression(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation:\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R² Score: {r2:.2f}\")\n",
        "    return mse, mae, r2\n",
        "\n",
        "mse1, mae1, r2_1 = evaluate_regression(dt1, Xr_test, yr_test, \"DT-1 Fully Grown\")\n",
        "mse2, mae2, r2_2 = evaluate_regression(dt2, Xr_test, yr_test, \"DT-2 Pruned (max_depth=4)\")\n",
        "\n",
        "# ============ VISUALIZATION OF TREES ============\n",
        "# Plot DT-1\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt1, filled=True, feature_names=Xr_train.columns)\n",
        "plt.title(\"Fully Grown Decision Tree (DT-1)\")\n",
        "plt.show()\n",
        "\n",
        "# Plot DT-2\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt2, filled=True, feature_names=Xr_train.columns)\n",
        "plt.title(\"Pruned Decision Tree (DT-2)\")\n",
        "plt.show()\n",
        "\n",
        "# ============ INTERPRETATION – PATIENT PREDICTION ============\n",
        "# Predict survival months for patient B002565 (manually construct input)\n",
        "patient = pd.DataFrame({\n",
        "    'Age': [29],\n",
        "    'Tumor_Size': [41],\n",
        "    'Regional_Node_Examined': [5],\n",
        "    'Regional_Node_Positive': [1],\n",
        "    'Sex_Male': [0],\n",
        "    'T_Stage_T3': [1],\n",
        "    'N_Stage_N1': [1],\n",
        "    '6th_Stage_IIIC': [1],\n",
        "    'Differentiated_Moderately differentiated': [1],\n",
        "    'A_Stage_Regional': [1],\n",
        "    'Estrogen_Status_Negative': [1],\n",
        "    'Progesterone_Status_Positive': [1]\n",
        "}, index=[0])  # All other columns default to 0\n",
        "\n",
        "# Align with training data columns\n",
        "patient = patient.reindex(columns=Xr_train.columns, fill_value=0)\n",
        "\n",
        "# Predict\n",
        "prediction_dt = dt2.predict(patient)\n",
        "print(f\"\\nPredicted Survival Months for patient B002565: {prediction_dt[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "_rro4TooOrrE",
        "outputId": "bf607709-e746-4575-f159-890ac78840c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'Survival_Months' not found in the dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input y contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c4fb85372aa2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train-Test Split for Classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mXc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Train-Test Split for Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2403\u001b[0m                 \u001b[0mUserWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2404\u001b[0m             )\n\u001b[0;32m-> 2405\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Title: Final Python Notebook 3 – Ensemble Classifier & DT Regression\n",
        "# Author: Suresh Thaheshan\n",
        "# Peer Reviewer: [Peer Reviewer Name], Date: [Review Date]\n",
        "# Reused from: Code Reuse Session 3\n",
        "# =============================================================\n",
        "\n",
        "# ============ IMPORT LIBRARIES ============\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_curve, auc,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# ============ LOAD DATA ============\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine_Learning_CourseWork/classification_dataset.csv')\n",
        "\n",
        "# Create classification and regression datasets\n",
        "df_class = df.drop(columns=['Survival_Months'])  # Classification dataset\n",
        "df_reg = df[df['Mortality_Status'] == 1].drop(columns=['Mortality_Status'])  # Regression dataset\n",
        "\n",
        "# ============ PREPROCESSING ============\n",
        "# One-hot encode categorical features if necessary\n",
        "X_class = pd.get_dummies(df_class.drop(columns=['Mortality_Status']), drop_first=True)\n",
        "y_class = df_class['Mortality_Status']\n",
        "\n",
        "X_reg = pd.get_dummies(df_reg.drop(columns=['Survival_Months']), drop_first=True)\n",
        "y_reg = df_reg['Survival_Months']\n",
        "\n",
        "# Train-Test Split\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_class, y_class, test_size=0.2, stratify=y_class, random_state=42)\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============ ENSEMBLE CLASSIFIER ============\n",
        "# Define base learners\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "nb = GaussianNB()\n",
        "# You can swap one of the models for KNN if desired\n",
        "\n",
        "# Ensemble using VotingClassifier (soft voting)\n",
        "ensemble_model = VotingClassifier(estimators=[('lr', lr), ('nb', nb)], voting='soft')\n",
        "\n",
        "# Train ensemble\n",
        "ensemble_model.fit(Xc_train, yc_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred_ens = ensemble_model.predict(Xc_test)\n",
        "y_prob_ens = ensemble_model.predict_proba(Xc_test)[:, 1]\n",
        "\n",
        "# Confusion matrix & classification report\n",
        "print(\"Confusion Matrix (Ensemble):\")\n",
        "print(confusion_matrix(yc_test, y_pred_ens))\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(yc_test, y_pred_ens))\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(yc_test, y_prob_ens)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc(fpr, tpr):.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Ensemble ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============ REGRESSION – DECISION TREE MODELS ============\n",
        "# Model 1: Fully grown Decision Tree\n",
        "dt1 = DecisionTreeRegressor(random_state=42)\n",
        "dt1.fit(Xr_train, yr_train)\n",
        "\n",
        "# Model 2: Pruned Decision Tree (max_depth=4)\n",
        "dt2 = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt2.fit(Xr_train, yr_train)\n",
        "\n",
        "# ============ EVALUATION – REGRESSION MODELS ============\n",
        "def evaluate_regression(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation:\")\n",
        "    print(f\"MSE: {mse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R² Score: {r2:.2f}\")\n",
        "    return mse, mae, r2\n",
        "\n",
        "mse1, mae1, r2_1 = evaluate_regression(dt1, Xr_test, yr_test, \"DT-1 Fully Grown\")\n",
        "mse2, mae2, r2_2 = evaluate_regression(dt2, Xr_test, yr_test, \"DT-2 Pruned (max_depth=4)\")\n",
        "\n",
        "# ============ VISUALIZATION OF TREES ============\n",
        "# Plot DT-1\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt1, filled=True, feature_names=Xr_train.columns)\n",
        "plt.title(\"Fully Grown Decision Tree (DT-1)\")\n",
        "plt.show()\n",
        "\n",
        "# Plot DT-2\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt2, filled=True, feature_names=Xr_train.columns)\n",
        "plt.title(\"Pruned Decision Tree (DT-2)\")\n",
        "plt.show()\n",
        "\n",
        "# ============ INTERPRETATION – PATIENT PREDICTION ============\n",
        "# Predict survival months for patient B002565 (manually construct input)\n",
        "patient = pd.DataFrame({\n",
        "    'Age': [29],\n",
        "    'Tumour_Size': [41],\n",
        "    'Regional_Node_Examined': [5],\n",
        "    'Regional_Node_Positive': [1],\n",
        "    'Month_of_Birth_July': [1],\n",
        "    'Sex_Male': [0],\n",
        "    'Occupation_15': [1],\n",
        "    'T_Stage_T3': [1],\n",
        "    'N_Stage_N1': [1],\n",
        "    'Stage_6th_IIIC': [1],\n",
        "    'Differentiated_Moderately differentiated': [1],\n",
        "    'Grade_2': [1],\n",
        "    'A_Stage_Regional': [1],\n",
        "    'Estrogen_Status_Negative': [1],\n",
        "    'Progesterone_Status_Positive': [1]\n",
        "}, index=[0])  # All other columns default to 0\n",
        "\n",
        "# Align with training data columns\n",
        "patient = patient.reindex(columns=Xr_train.columns, fill_value=0)\n",
        "\n",
        "# Predict\n",
        "prediction_dt = dt2.predict(patient)\n",
        "print(f\"\\nPredicted Survival Months for patient B002565: {prediction_dt[0]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Un1qTJzUN0Pk",
        "outputId": "ffeebf05-d907-40df-837a-09b035e056ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b581c54e097a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train-Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mXc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mXr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXr_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myr_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2319\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q_-BHTYgvzl",
        "outputId": "6561139c-ac2d-4a95-d320-d9cdf9b29e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading processed datasets...\n",
            "Classification dataset shape: (4019, 66)\n",
            "Regression dataset shape: (4019, 67)\n",
            "\n",
            "=== CHECKING FOR MISSING VALUES ===\n",
            "Classification dataset missing values: 1\n",
            "Regression dataset missing values: 1\n",
            "\n",
            "Columns with missing values in classification dataset:\n",
            "- Regional_Node_Examined: 1 missing values (0.02%)\n",
            "\n",
            "=== CLEANING MORTALITY STATUS VALUES ===\n",
            "\n",
            "Mortality Status values:\n",
            "Mortality_Status\n",
            "0        3395\n",
            "1         597\n",
            "DEAD       10\n",
            "dead        8\n",
            "ALIVE       5\n",
            "alive       3\n",
            "ALive       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Cleaned Mortality Status values:\n",
            "Mortality_Status\n",
            "0    3404\n",
            "1     615\n",
            "Name: count, dtype: int64\n",
            "Mortality_Status\n",
            "0    0.846977\n",
            "1    0.153023\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "=== CLASSIFICATION DATA PREPARATION ===\n",
            "Classification training set shape: (3215, 65)\n",
            "Classification testing set shape: (804, 65)\n",
            "\n",
            "=== ENSEMBLE CLASSIFIER ===\n",
            "\n",
            "--- Ensemble Classifier Evaluation ---\n",
            "\n",
            "Confusion Matrix:\n",
            "[[360 321]\n",
            " [ 29  94]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.53      0.67       681\n",
            "           1       0.23      0.76      0.35       123\n",
            "\n",
            "    accuracy                           0.56       804\n",
            "   macro avg       0.58      0.65      0.51       804\n",
            "weighted avg       0.82      0.56      0.62       804\n",
            "\n",
            "Accuracy Score: 0.5647\n",
            "\n",
            "=== REGRESSION DATA PREPARATION ===\n",
            "Regression training set shape: (3215, 65)\n",
            "Regression testing set shape: (804, 65)\n",
            "\n",
            "=== DECISION TREE REGRESSION MODELS ===\n",
            "\n",
            "=== DECISION TREE REGRESSION EVALUATION ===\n",
            "\n",
            "--- Full Decision Tree Evaluation ---\n",
            "Mean Squared Error: 1081.16\n",
            "Mean Absolute Error: 26.17\n",
            "R² Score: -1.0208\n",
            "\n",
            "--- Pruned Decision Tree Evaluation ---\n",
            "Mean Squared Error: 533.38\n",
            "Mean Absolute Error: 18.61\n",
            "R² Score: 0.0030\n",
            "\n",
            "--- Gradient Boosting Evaluation ---\n",
            "Mean Squared Error: 539.86\n",
            "Mean Absolute Error: 18.67\n",
            "R² Score: -0.0091\n",
            "\n",
            "=== MODEL COMPARISON SUMMARY ===\n",
            "\n",
            "Classification Model (Ensemble):\n",
            "Accuracy Score: 0.5647\n",
            "AUC: 0.7237\n",
            "\n",
            "Regression Models:\n",
            "Full Decision Tree - MSE: 1081.16, R²: -1.0208\n",
            "Pruned Decision Tree - MSE: 533.38, R²: 0.0030\n",
            "Gradient Boosting - MSE: 539.86, R²: -0.0091\n",
            "\n",
            "=== SAVING MODELS ===\n",
            "\n",
            "All models saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# =============================================================\n",
        "# Title: Notebook 3 – Ensemble Classifier & Decision Tree Regression\n",
        "# Author: Suresh Thaheshan\n",
        "# Peer Reviewer: Ayman Jaleel\n",
        "# Date: May 3, 2025\n",
        "# =============================================================\n",
        "\n",
        "# === 1. IMPORT LIBRARIES ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import VotingClassifier, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_curve, auc,\n",
        "    mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import SimpleImputer  # Import imputer for handling missing values\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Define save path\n",
        "save_path = '/content/drive/MyDrive/Machine_Learning_CourseWork'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# === 2. LOAD DATA ===\n",
        "print(\"Loading processed datasets...\")\n",
        "classification_df = pd.read_csv(os.path.join(save_path, 'classification_dataset.csv'))\n",
        "regression_df = pd.read_csv(os.path.join(save_path, 'regression_dataset.csv'))\n",
        "\n",
        "print(f\"Classification dataset shape: {classification_df.shape}\")\n",
        "print(f\"Regression dataset shape: {regression_df.shape}\")\n",
        "\n",
        "# === 3. CHECK FOR MISSING VALUES ===\n",
        "print(\"\\n=== CHECKING FOR MISSING VALUES ===\")\n",
        "print(f\"Classification dataset missing values: {classification_df.isna().sum().sum()}\")\n",
        "print(f\"Regression dataset missing values: {regression_df.isna().sum().sum()}\")\n",
        "\n",
        "# Print columns with missing values\n",
        "print(\"\\nColumns with missing values in classification dataset:\")\n",
        "missing_cols = classification_df.columns[classification_df.isna().any()].tolist()\n",
        "for col in missing_cols:\n",
        "    missing_count = classification_df[col].isna().sum()\n",
        "    print(f\"- {col}: {missing_count} missing values ({missing_count/len(classification_df)*100:.2f}%)\")\n",
        "\n",
        "# === 4. CLEAN UP MORTALITY STATUS VALUES ===\n",
        "print(\"\\n=== CLEANING MORTALITY STATUS VALUES ===\")\n",
        "\n",
        "# Check the current values\n",
        "print(\"\\nMortality Status values:\")\n",
        "print(classification_df['Mortality_Status'].value_counts())\n",
        "\n",
        "# Clean up the mortality status values - convert to binary\n",
        "mortality_mapping = {\n",
        "    0: 0, '0': 0, 'ALIVE': 0, 'alive': 0, 'ALive': 0, 'Alive': 0,\n",
        "    1: 1, '1': 1, 'DEAD': 1, 'dead': 1, 'Dead': 1\n",
        "}\n",
        "\n",
        "# Apply mapping to both datasets\n",
        "classification_df['Mortality_Status'] = classification_df['Mortality_Status'].map(mortality_mapping)\n",
        "regression_df['Mortality_Status'] = regression_df['Mortality_Status'].map(mortality_mapping)\n",
        "\n",
        "# Convert to integer type\n",
        "classification_df['Mortality_Status'] = classification_df['Mortality_Status'].astype(int)\n",
        "regression_df['Mortality_Status'] = regression_df['Mortality_Status'].astype(int)\n",
        "\n",
        "print(\"\\nCleaned Mortality Status values:\")\n",
        "print(classification_df['Mortality_Status'].value_counts())\n",
        "print(classification_df['Mortality_Status'].value_counts(normalize=True))\n",
        "\n",
        "# === 5. CLASSIFICATION DATA PREPARATION ===\n",
        "print(\"\\n=== CLASSIFICATION DATA PREPARATION ===\")\n",
        "\n",
        "# 5.1 Setup features and target for classification\n",
        "# Corrected column name from 'Mortality_Status_1' to 'Mortality_Status'\n",
        "X_class = classification_df.drop(columns=['Mortality_Status'])\n",
        "y_class = classification_df['Mortality_Status']\n",
        "\n",
        "# 5.2 Train-test split for classification\n",
        "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, stratify=y_class, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Classification training set shape: {X_train_class.shape}\")\n",
        "print(f\"Classification testing set shape: {X_test_class.shape}\")\n",
        "\n",
        "# 5.3 Create a pipeline for preprocessing with imputation and scaling\n",
        "preprocessing_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
        "    ('scaler', StandardScaler())  # Scale features\n",
        "])\n",
        "\n",
        "# Transform the data\n",
        "X_train_class_processed = preprocessing_pipeline.fit_transform(X_train_class)\n",
        "X_test_class_processed = preprocessing_pipeline.transform(X_test_class)\n",
        "\n",
        "# === 6. ENSEMBLE CLASSIFIER ===\n",
        "print(\"\\n=== ENSEMBLE CLASSIFIER ===\")\n",
        "\n",
        "# 6.1 Define base learners\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
        "naive_bayes = GaussianNB()\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# 6.2 Create and train voting ensemble (soft voting)\n",
        "ensemble_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', log_reg),\n",
        "        ('nb', naive_bayes),\n",
        "        ('knn', knn)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "ensemble_classifier.fit(X_train_class_processed, y_train_class)\n",
        "\n",
        "# 6.3 Make predictions\n",
        "y_pred_ensemble = ensemble_classifier.predict(X_test_class_processed)\n",
        "y_prob_ensemble = ensemble_classifier.predict_proba(X_test_class_processed)[:, 1]\n",
        "\n",
        "# 6.4 Evaluate ensemble classifier\n",
        "print(\"\\n--- Ensemble Classifier Evaluation ---\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_class, y_pred_ensemble))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_class, y_pred_ensemble))\n",
        "\n",
        "print(f\"Accuracy Score: {accuracy_score(y_test_class, y_pred_ensemble):.4f}\")\n",
        "\n",
        "# 6.5 ROC Curve and AUC\n",
        "fpr_ens, tpr_ens, _ = roc_curve(y_test_class, y_prob_ensemble)\n",
        "auc_ens = auc(fpr_ens, tpr_ens)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC = {auc_ens:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Ensemble Classifier')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(save_path, 'ensemble_roc_curve.png'))\n",
        "plt.close()\n",
        "\n",
        "# 6.6 Confusion Matrix Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(confusion_matrix(y_test_class, y_pred_ensemble), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Ensemble Classifier')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig(os.path.join(save_path, 'ensemble_confusion_matrix.png'))\n",
        "plt.close()\n",
        "\n",
        "# === 7. REGRESSION DATA PREPARATION ===\n",
        "print(\"\\n=== REGRESSION DATA PREPARATION ===\")\n",
        "\n",
        "# 7.1 Setup features and target for regression\n",
        "# Corrected column name from 'Mortality_Status_1' to 'Mortality_Status'\n",
        "X_reg = regression_df.drop(columns=['Survival_Months', 'Mortality_Status'])\n",
        "y_reg = regression_df['Survival_Months']\n",
        "\n",
        "# 7.2 Train-test split for regression\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Regression training set shape: {X_train_reg.shape}\")\n",
        "print(f\"Regression testing set shape: {X_test_reg.shape}\")\n",
        "\n",
        "# 7.3 Create a pipeline for preprocessing with imputation\n",
        "reg_preprocessing_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median'))  # Handle missing values\n",
        "])\n",
        "\n",
        "# Transform the data\n",
        "X_train_reg_processed = reg_preprocessing_pipeline.fit_transform(X_train_reg)\n",
        "X_test_reg_processed = reg_preprocessing_pipeline.transform(X_test_reg)\n",
        "\n",
        "# === 8. DECISION TREE REGRESSION MODELS ===\n",
        "print(\"\\n=== DECISION TREE REGRESSION MODELS ===\")\n",
        "\n",
        "# 8.1 Model 1: Full Decision Tree (no constraint)\n",
        "dt_reg_full = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg_full.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# 8.2 Model 2: Pruned Decision Tree\n",
        "dt_reg_pruned = DecisionTreeRegressor(max_depth=4, min_samples_split=10, random_state=42)\n",
        "dt_reg_pruned.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# 8.3 Gradient Boosting Regressor (additional model)\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_reg.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# 8.4 Make predictions\n",
        "y_pred_dt_full = dt_reg_full.predict(X_test_reg_processed)\n",
        "y_pred_dt_pruned = dt_reg_pruned.predict(X_test_reg_processed)\n",
        "y_pred_gb = gb_reg.predict(X_test_reg_processed)\n",
        "\n",
        "# === 9. DECISION TREE REGRESSION EVALUATION ===\n",
        "print(\"\\n=== DECISION TREE REGRESSION EVALUATION ===\")\n",
        "\n",
        "# 9.1 Evaluate Full Decision Tree\n",
        "mse_dt_full = mean_squared_error(y_test_reg, y_pred_dt_full)\n",
        "mae_dt_full = mean_absolute_error(y_test_reg, y_pred_dt_full)\n",
        "r2_dt_full = r2_score(y_test_reg, y_pred_dt_full)\n",
        "\n",
        "print(\"\\n--- Full Decision Tree Evaluation ---\")\n",
        "print(f\"Mean Squared Error: {mse_dt_full:.2f}\")\n",
        "print(f\"Mean Absolute Error: {mae_dt_full:.2f}\")\n",
        "print(f\"R² Score: {r2_dt_full:.4f}\")\n",
        "\n",
        "# 9.2 Evaluate Pruned Decision Tree\n",
        "mse_dt_pruned = mean_squared_error(y_test_reg, y_pred_dt_pruned)\n",
        "mae_dt_pruned = mean_absolute_error(y_test_reg, y_pred_dt_pruned)\n",
        "r2_dt_pruned = r2_score(y_test_reg, y_pred_dt_pruned)\n",
        "\n",
        "print(\"\\n--- Pruned Decision Tree Evaluation ---\")\n",
        "print(f\"Mean Squared Error: {mse_dt_pruned:.2f}\")\n",
        "print(f\"Mean Absolute Error: {mae_dt_pruned:.2f}\")\n",
        "print(f\"R² Score: {r2_dt_pruned:.4f}\")\n",
        "\n",
        "# 9.3 Evaluate Gradient Boosting\n",
        "mse_gb = mean_squared_error(y_test_reg, y_pred_gb)\n",
        "mae_gb = mean_absolute_error(y_test_reg, y_pred_gb)\n",
        "r2_gb = r2_score(y_test_reg, y_pred_gb)\n",
        "\n",
        "print(\"\\n--- Gradient Boosting Evaluation ---\")\n",
        "print(f\"Mean Squared Error: {mse_gb:.2f}\")\n",
        "print(f\"Mean Absolute Error: {mae_gb:.2f}\")\n",
        "print(f\"R² Score: {r2_gb:.4f}\")\n",
        "\n",
        "# 9.4 Visualize Decision Tree (Pruned for better interpretability)\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_reg_pruned,\n",
        "          feature_names=X_train_reg.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          max_depth=3)  # Limiting depth for visualization\n",
        "plt.title(\"Pruned Decision Tree (Max Depth=3)\")\n",
        "plt.savefig(os.path.join(save_path, 'decision_tree_visualization.png'), dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 9.5 Feature Importance for Gradient Boosting\n",
        "feature_importance_gb = pd.DataFrame({\n",
        "    'Feature': X_train_reg.columns,\n",
        "    'Importance': gb_reg.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance_gb.head(20))\n",
        "plt.title('Top 20 Gradient Boosting Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_path, 'gb_feature_importance.png'))\n",
        "plt.close()\n",
        "\n",
        "# 9.6 Actual vs Predicted comparison for all models\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Full Decision Tree\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test_reg, y_pred_dt_full, alpha=0.7)\n",
        "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=2)\n",
        "plt.title('Full Decision Tree: Actual vs Predicted')\n",
        "plt.xlabel('Actual Survival Months')\n",
        "plt.ylabel('Predicted Survival Months')\n",
        "plt.grid(True)\n",
        "\n",
        "# Pruned Decision Tree\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_test_reg, y_pred_dt_pruned, alpha=0.7, color='orange')\n",
        "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=2)\n",
        "plt.title('Pruned Decision Tree: Actual vs Predicted')\n",
        "plt.xlabel('Actual Survival Months')\n",
        "plt.ylabel('Predicted Survival Months')\n",
        "plt.grid(True)\n",
        "\n",
        "# Gradient Boosting\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_test_reg, y_pred_gb, alpha=0.7, color='green')\n",
        "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--', lw=2)\n",
        "plt.title('Gradient Boosting: Actual vs Predicted')\n",
        "plt.xlabel('Actual Survival Months')\n",
        "plt.ylabel('Predicted Survival Months')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(save_path, 'regression_models_comparison.png'))\n",
        "plt.close()\n",
        "\n",
        "# === 10. MODEL COMPARISON AND SUMMARY ===\n",
        "print(\"\\n=== MODEL COMPARISON SUMMARY ===\")\n",
        "\n",
        "# 10.1 Classification Summary\n",
        "print(\"\\nClassification Model (Ensemble):\")\n",
        "print(f\"Accuracy Score: {accuracy_score(y_test_class, y_pred_ensemble):.4f}\")\n",
        "print(f\"AUC: {auc_ens:.4f}\")\n",
        "\n",
        "# 10.2 Regression Summary\n",
        "print(\"\\nRegression Models:\")\n",
        "print(f\"Full Decision Tree - MSE: {mse_dt_full:.2f}, R²: {r2_dt_full:.4f}\")\n",
        "print(f\"Pruned Decision Tree - MSE: {mse_dt_pruned:.2f}, R²: {r2_dt_pruned:.4f}\")\n",
        "print(f\"Gradient Boosting - MSE: {mse_gb:.2f}, R²: {r2_gb:.4f}\")\n",
        "\n",
        "# 10.3 Save the best models\n",
        "print(\"\\n=== SAVING MODELS ===\")\n",
        "joblib.dump(ensemble_classifier, os.path.join(save_path, 'ensemble_classifier.joblib'))\n",
        "joblib.dump(preprocessing_pipeline, os.path.join(save_path, 'ensemble_preprocessing_pipeline.joblib'))\n",
        "\n",
        "joblib.dump(gb_reg, os.path.join(save_path, 'gradient_boosting_regressor.joblib'))\n",
        "joblib.dump(reg_preprocessing_pipeline, os.path.join(save_path, 'regression_preprocessing_pipeline.joblib'))\n",
        "\n",
        "print(\"\\nAll models saved successfully.\")"
      ]
    }
  ]
}